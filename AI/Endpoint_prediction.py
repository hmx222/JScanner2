import os
import logging
import re
import time
import random
from io import StringIO
from collections import deque
from itertools import chain

try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity

    SEMANTIC_ANALYSIS_AVAILABLE = True
    _semantic_model = None
except ImportError:
    SEMANTIC_ANALYSIS_AVAILABLE = False
    _semantic_model = None

# OLLAMA GPUå†…å­˜é™åˆ¶ï¼Œæ ¹æ®æœ¬åœ°æ˜¾å¡å†…å­˜è°ƒæ•´
OLLAMA_GPU_MEMORY = "4GB"

# æ—¥å¿—çº§åˆ«è®¾ç½®
LANGCHAIN_LOG_LEVEL = logging.ERROR
HTTPX_LOG_LEVEL = logging.ERROR

# è°ƒç”¨çš„OLLAMAæ¨¡å‹åç§°
MODEL_NAME = "qwen2.5:14b-instruct-q2_K"

# æ¨¡å‹ç”Ÿæˆå‚æ•°
MODEL_TEMPERATURE = 0.6
MODEL_MAX_TOKENS = 300  # å‡å°‘æœ€å¤§ä»¤ç‰Œæ•°ï¼Œé€‚åˆçŸ­è¾“å‡º

# å¾ªç¯æ£€æµ‹å‚æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
LOOP_PROTECTION_TOKEN_WINDOW = 30
LOOP_PROTECTION_MAX_TOKEN_REPEAT = 4
LOOP_PROTECTION_SENTENCE_WINDOW = 5
LOOP_PROTECTION_SIMILARITY_THRESHOLD = 0.82
LOOP_PROTECTION_CHECK_INTERVAL = 5

# æ¢å¤ç­–ç•¥æƒé‡
LOOP_PROTECTION_RECOVERY_STRATEGY = {
    "increase_temperature": 0.6,
    "inject_diversity": 0.3,
    "hard_terminate": 0.1
}

logging.getLogger("langchain").setLevel(LANGCHAIN_LOG_LEVEL)
logging.getLogger("httpx").setLevel(HTTPX_LOG_LEVEL)
os.environ["OLLAMA_GPU_MEMORY"] = OLLAMA_GPU_MEMORY

from langchain_community.chat_models import ChatOllama
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.prompts import PromptTemplate


def get_semantic_model():
    """å»¶è¿ŸåŠ è½½è¯­ä¹‰æ¨¡å‹"""
    global _semantic_model

    if _semantic_model is not None:
        return _semantic_model

    if not SEMANTIC_ANALYSIS_AVAILABLE:
        _semantic_model = "SIMPLE"
        return _semantic_model

    try:
        try:
            from modelscope import snapshot_download
            from sentence_transformers import SentenceTransformer
        except ImportError:
            _semantic_model = "SIMPLE"
            return _semantic_model

        os.environ["MODELSCOPE_CACHE"] = "./modelscope_models"
        model_dir = snapshot_download(
            'Ceceliachenen/paraphrase-multilingual-MiniLM-L12-v2',
            cache_dir='./modelscope_models',
            revision='master'
        )

        _semantic_model = SentenceTransformer(model_dir)
        logging.info("æˆåŠŸåŠ è½½è¯­ä¹‰åˆ†ææ¨¡å‹")
        return _semantic_model

    except Exception as e:
        logging.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨ç®€å•ç›¸ä¼¼åº¦æ£€æµ‹")
        _semantic_model = "SIMPLE"
        return _semantic_model


class LoopProtectionCallback(BaseCallbackHandler):
    """å¾ªç¯æ£€æµ‹ä¸é˜²æŠ¤å›è°ƒå¤„ç†å™¨"""

    def __init__(self,
                 token_window=LOOP_PROTECTION_TOKEN_WINDOW,
                 max_token_repeat=LOOP_PROTECTION_MAX_TOKEN_REPEAT,
                 sentence_window=LOOP_PROTECTION_SENTENCE_WINDOW,
                 similarity_threshold=LOOP_PROTECTION_SIMILARITY_THRESHOLD,
                 check_interval=LOOP_PROTECTION_CHECK_INTERVAL):
        self.buffer = StringIO()
        self.token_count = 0
        self.last_tokens = deque(maxlen=token_window)
        self.token_repetition_count = 0
        self.sentence_history = deque(maxlen=sentence_window)
        self.current_sentence = []
        self.sentence_similarity_checks = 0
        self.check_interval = check_interval
        self.similarity_threshold = similarity_threshold
        self.loop_detected = False
        self.termination_phrase = "\n\n[å†…å®¹ç”Ÿæˆå·²å› æ£€æµ‹åˆ°é‡å¤æ¨¡å¼è€Œç»ˆæ­¢]"

        self.token_window = token_window
        self.max_token_repeat = max_token_repeat
        self.sentence_window = sentence_window

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """å¤„ç†æ–°ç”Ÿæˆçš„tokenå¹¶æ£€æµ‹å¾ªç¯"""
        self.buffer.write(token)
        self.token_count += 1
        self.last_tokens.append(token)

        print(token, end="", flush=True)

        if self.token_count % self.check_interval == 0:
            self._check_for_loop()

    def get_output(self) -> str:
        """è·å–ç”Ÿæˆçš„è¾“å‡ºå†…å®¹"""
        return self.buffer.getvalue().strip()

    def _check_for_loop(self):
        """æ‰§è¡Œå¤šå±‚å¾ªç¯æ£€æµ‹"""
        if self.loop_detected:
            return

        # L1: è¯çº§æ£€æµ‹
        if self._check_token_repetition():
            self._handle_loop_detection("token_repetition")
            return

        # L2: å¥çº§æ£€æµ‹
        if self.token_count > 50 and self._check_sentence_similarity():
            self._handle_loop_detection("sentence_similarity")
            return

    def _check_token_repetition(self) -> bool:
        """æ£€æŸ¥tokençº§åˆ«é‡å¤"""
        pattern_len = self.token_window // 2
        if pattern_len > 0:
            first_half = ''.join(list(self.last_tokens)[:pattern_len])
            second_half = ''.join(list(self.last_tokens)[pattern_len:])

            if first_half and first_half in second_half:
                self.token_repetition_count += 1
                return self.token_repetition_count >= self.max_token_repeat

        return False

    def _check_sentence_similarity(self) -> bool:
        """æ£€æŸ¥å¥å­çº§åˆ«è¯­ä¹‰é‡å¤"""
        if not self.current_sentence:
            self.current_sentence = []

        if self.last_tokens[-1] in ['ã€‚', '!', '?', '\n', '.', '!', '?']:
            current_sentence_text = ''.join(self.current_sentence).strip()
            if len(current_sentence_text) > 10 and self.sentence_history:
                max_similarity = self._calculate_similarity(current_sentence_text,
                                                            list(self.sentence_history))
                if max_similarity > self.similarity_threshold:
                    self.sentence_similarity_checks += 1
                    return self.sentence_similarity_checks >= 2

                self.sentence_history.append(current_sentence_text)

            self.current_sentence = []
        else:
            self.current_sentence.append(self.last_tokens[-1])

        return False

    def _calculate_similarity(self, text1, texts):
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        if not texts:
            return 0.0

        if not SEMANTIC_ANALYSIS_AVAILABLE or get_semantic_model() == "SIMPLE":
            return self._simple_similarity(text1, texts)

        try:
            model = get_semantic_model()
            if model == "SIMPLE":
                return self._simple_similarity(text1, texts)

            embeddings = model.encode([text1] + texts)
            text1_embed = embeddings[0].reshape(1, -1)
            max_sim = 0

            for i in range(1, len(embeddings)):
                sim = cosine_similarity(text1_embed, embeddings[i].reshape(1, -1))[0][0]
                max_sim = max(max_sim, sim)

            return max_sim
        except Exception as e:
            logging.warning(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å‡ºé”™: {str(e)}ï¼Œä½¿ç”¨ç®€å•æ£€æµ‹")
            return self._simple_similarity(text1, texts)

    def _simple_similarity(self, text1, texts):
        """ç®€å•Jaccardç›¸ä¼¼åº¦è®¡ç®—"""
        set1 = set(text1)
        max_sim = 0

        for text2 in texts:
            set2 = set(text2)
            intersection = len(set1 & set2)
            union = len(set1 | set2)
            sim = intersection / union if union > 0 else 0
            max_sim = max(max_sim, sim)

        return max_sim

    def _handle_loop_detection(self, detection_type):
        """å¤„ç†æ£€æµ‹åˆ°çš„å¾ªç¯"""
        self.loop_detected = True
        print(f"\næ£€æµ‹åˆ°{detection_type}å¾ªç¯ï¼Œå°è¯•æ¢å¤...", flush=True)

        strategy = self._select_recovery_strategy()

        if strategy == "increase_temperature":
            self.buffer.write("\n\næ¢ä¸ªè§’åº¦æ€è€ƒè¿™ä¸ªAPIç«¯ç‚¹çš„ç›¸å…³æ“ä½œ...\n")
        elif strategy == "inject_diversity":
            diversions = [
                "è¿™ä¸ªAPIç«¯ç‚¹å¯èƒ½æœ‰å…¶ä»–ç›¸å…³çš„æ“ä½œï¼Œæ¯”å¦‚...",
                "ä»åŠŸèƒ½ç›¸åçš„è§’åº¦çœ‹ï¼Œå¯èƒ½å­˜åœ¨...",
                "ç±»ä¼¼çš„APIè®¾è®¡ä¸­é€šå¸¸è¿˜ä¼šåŒ…å«...",
            ]
            self.buffer.write(f"\n\n{random.choice(diversions)}\n")
        else:
            self.buffer.write(self.termination_phrase)
            raise Exception("Loop detection triggered termination")

    def _select_recovery_strategy(self):
        """é€‰æ‹©æ¢å¤ç­–ç•¥"""
        strategies = list(LOOP_PROTECTION_RECOVERY_STRATEGY.keys())
        weights = list(LOOP_PROTECTION_RECOVERY_STRATEGY.values())
        return random.choices(strategies, weights=weights, k=1)[0]


def load_ollama_llm():
    """åŠ è½½OLLAMAæ¨¡å‹"""
    return ChatOllama(
        model=MODEL_NAME,
        temperature=MODEL_TEMPERATURE,
        max_tokens=MODEL_MAX_TOKENS,
        streaming=True,
        keep_alive=-1
    )


def build_analysis_chain(llm):
    """æ„å»ºAPIç«¯ç‚¹é¢„æµ‹é“¾ï¼ˆä½¿ç”¨ä½ çš„æç¤ºè¯ï¼‰"""
    prompt_template = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ¸—é€æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œä½ çš„ä»»åŠ¡æ˜¯ç»“åˆæ¸—é€æµ‹è¯•çš„æ€æƒ³ï¼Œä»ç»™å®šçš„APIåˆ—è¡¨ä¸­æ¨æµ‹å‡ºæ›´å¤šçš„APIï¼Œç”¨äºè¾…åŠ©æ¸—é€æµ‹è¯•ã€‚
è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ç°æœ‰çš„APIåˆ—è¡¨ï¼š
<APIåˆ—è¡¨>
{{API_LIST}}
</APIåˆ—è¡¨>
åœ¨æ¨æµ‹APIæ—¶ï¼Œè¯·éµå¾ªä»¥ä¸‹æ€è·¯å’Œæ–¹æ³•ï¼š
1. åˆ†æç°æœ‰APIçš„å‘½åè§„åˆ™ã€åŠŸèƒ½ç‰¹ç‚¹ã€å‚æ•°ç»“æ„ç­‰ï¼Œæ‰¾å‡ºå…¶ä¸­çš„è§„å¾‹å’Œæ¨¡å¼ã€‚
2. è€ƒè™‘å¸¸è§çš„ä¸šåŠ¡é€»è¾‘å’Œæ“ä½œæµç¨‹ï¼Œæ¨æµ‹å¯èƒ½ä¸ä¹‹ç›¸å…³çš„å…¶ä»–APIã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ä¸€ä¸ªè·å–ç”¨æˆ·ä¿¡æ¯çš„APIï¼Œå¯èƒ½å­˜åœ¨æ›´æ–°ç”¨æˆ·ä¿¡æ¯ã€åˆ é™¤ç”¨æˆ·ä¿¡æ¯ç­‰ç›¸å…³APIã€‚
3. æ€è€ƒAPIçš„æƒé™çº§åˆ«å’Œä½¿ç”¨åœºæ™¯ï¼Œæ¨æµ‹ä¸åŒæƒé™ä¸‹å¯èƒ½å­˜åœ¨çš„APIã€‚
4. ç»“åˆæ¸—é€æµ‹è¯•çš„ç»éªŒï¼Œè€ƒè™‘å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„è–„å¼±ç¯èŠ‚ï¼Œæ¨æµ‹ä¸ä¹‹å¯¹åº”çš„APIã€‚

å¦‚æœè§‰å¾—å½“å‰çš„APIåˆ—è¡¨æ²¡æœ‰é¢„æµ‹çš„ä»·å€¼ï¼Œé‚£ä¹ˆå¯ä»¥ä¸é¢„æµ‹ï¼Œç›´æ¥è¾“å‡ºNULLå³å¯ã€‚
æ¨æµ‹çš„APIè¦ä¸åŸAPIä¿æŒ80%ç»“æ„ç›¸ä¼¼åº¦ï¼Œå¦åˆ™è¾“å‡ºNULLã€‚

<æ¨æµ‹API>
[åœ¨æ­¤åˆ—å‡ºä½ æ¨æµ‹å‡ºçš„API]
</æ¨æµ‹API>
è¯·ç¡®ä¿ä½ çš„æ¨æµ‹åŸºäºåˆç†çš„åˆ†æå’Œæ¸—é€æµ‹è¯•çš„æ€æƒ³ã€‚
        """
    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["api_list"]  # æ˜ç¡®ä½¿ç”¨api_endpointä½œä¸ºå˜é‡
    )
    return prompt | llm


def analyze_api_endpoint(chain, api_endpoint):
    """åˆ†æå•ä¸ªAPIç«¯ç‚¹å¹¶ç”Ÿæˆé¢„æµ‹ç»“æœ"""
    protection_callback = LoopProtectionCallback(
        token_window=LOOP_PROTECTION_TOKEN_WINDOW,
        max_token_repeat=LOOP_PROTECTION_MAX_TOKEN_REPEAT,
        sentence_window=LOOP_PROTECTION_SENTENCE_WINDOW,
        similarity_threshold=LOOP_PROTECTION_SIMILARITY_THRESHOLD,
        check_interval=LOOP_PROTECTION_CHECK_INTERVAL
    )

    try:
        # ä¼ å…¥api_endpointå‚æ•°ï¼ˆè€Œécodeï¼‰
        chain.invoke(
            {"api_endpoint": api_endpoint},
            config={"callbacks": [protection_callback]}
        )
        return protection_callback.get_output()
    except Exception as e:
        if "Loop detection triggered termination" in str(e):
            return protection_callback.get_output()
        raise

def run_analysis(api_endpoints):
    """è¿è¡ŒAPIç«¯ç‚¹é¢„æµ‹æµç¨‹ï¼ˆä¸€æ¬¡æ€§ä¼ å…¥æ‰€æœ‰APIï¼‰"""
    llm = load_ollama_llm()
    analysis_chain = build_analysis_chain(llm)

    # å°†åˆ—è¡¨æ‹¼æ¥æˆå­—ç¬¦ä¸²ï¼Œæ¯è¡Œä¸€ä¸ªAPI
    api_list_str = "\n".join(api_endpoints)

    print(f"\nğŸ” ä¸€æ¬¡æ€§åˆ†æ {len(api_endpoints)} ä¸ª API ç«¯ç‚¹ï¼š")
    for api in api_endpoints:
        print(f"   â€¢ {api}")

    protection_callback = LoopProtectionCallback(
        token_window=LOOP_PROTECTION_TOKEN_WINDOW,
        max_token_repeat=LOOP_PROTECTION_MAX_TOKEN_REPEAT,
        sentence_window=LOOP_PROTECTION_SENTENCE_WINDOW,
        similarity_threshold=LOOP_PROTECTION_SIMILARITY_THRESHOLD,
        check_interval=LOOP_PROTECTION_CHECK_INTERVAL
    )

    try:
        chain.invoke(
            {"api_list": api_list_str},  # âš ï¸ ä¼ å…¥ api_list
            config={"callbacks": [protection_callback]}
        )
        model_output = protection_callback.get_output()
        cleaned = clean_output(model_output)
        return { "input_apis": api_endpoints, "predicted_apis": cleaned }

    except Exception as e:
        if "Loop detection triggered termination" in str(e):
            model_output = protection_callback.get_output()
            cleaned = clean_output(model_output)
            return { "input_apis": api_endpoints, "predicted_apis": cleaned }
        else:
            raise

def clean_output(output):
    """æ¸…ç†æ¨¡å‹è¾“å‡ºï¼Œæå–é¢„æµ‹çš„APIç«¯ç‚¹"""
    # ä¿®æ­£æ­£åˆ™åŒ¹é…ï¼Œé€‚é…<STR>å’Œ<END>æ ‡ç­¾
    paths = re.findall(r'<STR>(.*?)<END>', output, re.DOTALL)
    if not paths:
        return ["NULL"]  # æœªæ‰¾åˆ°ç»“æœæ—¶è¿”å›NULL

    # å¤„ç†æå–çš„å†…å®¹ï¼ˆæŒ‰è¡Œåˆ†å‰²ï¼Œå»é‡ï¼Œè¿‡æ»¤æ— æ•ˆå†…å®¹ï¼‰
    all_paths = []
    for path_block in paths:
        lines = [line.strip() for line in path_block.splitlines() if line.strip()]
        all_paths.extend(lines)

    # å»é‡
    unique_paths = list(set(all_paths))

    # è¿‡æ»¤æ— æ•ˆè·¯å¾„ï¼ˆä¿ç•™NULLå’Œç¬¦åˆAPIå‘½åè§„åˆ™çš„è·¯å¾„ï¼‰
    allowed_pattern = re.compile(r'^[a-zA-Z0-9_/-]+$|^NULL$')
    filtered_paths = [path for path in unique_paths if allowed_pattern.match(path)]

    # é™åˆ¶æœ€å¤š2ä¸ªç»“æœ
    return filtered_paths[:2] if filtered_paths else ["NULL"]


if __name__ == '__main__':
    print("ğŸ“Œ è¯·è¾“å…¥ä¸€ä¸ªæˆ–å¤šä¸ª API ç«¯ç‚¹ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œæˆ–ç”¨é€—å·åˆ†éš”ï¼Œè¾“å…¥ q é€€å‡ºï¼‰ï¼š")

    while True:
        user_input = input().strip()
        if user_input.lower() == 'q':
            break
        if not user_input:
            print("âš ï¸  è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªæœ‰æ•ˆçš„ API ç«¯ç‚¹")
            continue

        # æ”¯æŒé€—å·åˆ†éš”æˆ–æ¢è¡Œè¾“å…¥ï¼ˆå¦‚æœæ˜¯ç²˜è´´å¤šè¡Œï¼‰
        if '\n' in user_input:
            api_list = [line.strip() for line in user_input.splitlines() if line.strip()]
        else:
            api_list = [item.strip() for item in user_input.split(',') if item.strip()]

        if not api_list:
            print("âš ï¸  æœªæ£€æµ‹åˆ°æœ‰æ•ˆ API ç«¯ç‚¹")
            continue

        print(f"\nğŸš€ å¼€å§‹åˆ†æ {len(api_list)} ä¸ª API ç«¯ç‚¹...\n")

        try:
            all_results = run_analysis(api_list)

            print("\n" + "="*60)
            print("âœ… æœ€ç»ˆé¢„æµ‹ç»“æœæ±‡æ€»ï¼š")
            print("="*60)

            for api, predictions in all_results.items():
                print(f"\nğŸ”¹ åŸå§‹ API: {api}")
                for i, pred in enumerate(predictions, 1):
                    print(f"  {i}. {pred}")

        except Exception as e:
            print(f"âŒ æ•´ä½“å¤„ç†å‡ºé”™ï¼š{str(e)}")