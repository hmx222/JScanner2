import json
import math
import random
import re
import sys
from collections import Counter
from collections import OrderedDict

import nltk
from bs4 import BeautifulSoup
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from nltk.corpus import wordnet
from nltk.corpus import words
from pybloom_live import ScalableBloomFilter
from tqdm import tqdm

from AI.beautifyjs import format_code
from config import config

try:
    import wordninja
except ImportError:
    print("âš ï¸  ç¼ºå°‘ä¾èµ–åº“ 'wordninja'ï¼Œè¯·è¿è¡Œ: pip install wordninja")
    sys.exit(1)

try:
    from langchain_community.chat_models import ChatOllama
    from langchain_core.messages import SystemMessage, HumanMessage
except ImportError:
    print("âš ï¸  ç¼ºå°‘ langchain ä¾èµ–åº“ï¼Œè¯·è¿è¡Œ: pip install langchain-community langchain-core")
    sys.exit(1)

# åŠ è½½è¯åº“
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)
nltk.download('words', quiet=True)
nltk.data.path.append('../config/nltk_data')

# ==================== ç¬¬ä¸€æ­¥ï¼šç²—è¿‡æ»¤å™¨ ====================
class CodeLineFilter:
    def __init__(self,
                 min_string_length=5,
                 min_sensitive_length=5,
                 max_string_length=1000):
        self.min_string_length = min_string_length
        self.min_sensitive_length = min_sensitive_length
        self.max_string_length = max_string_length

        self.EXCLUDE_CONTEXTS = {
            'console.log', 'console.warn', 'console.error', 'console.info', 'console.debug',
            'alert(', 'confirm(', 'prompt(',
            'logger.log', 'logger.debug', 'logger.info', 'logger.warn', 'logger.error',
            'import ', 'require(', 'export ',
            'getElementById', 'querySelector', 'querySelectorAll',
            'getElementsByTagName', 'getElementsByClassName',
            'createElement', 'appendChild', 'innerHTML', 'textContent',
            '.css(', '.html(', '.text(', '.val(', '.attr(',
            '<div', '<span', '<a ', '<img', '<link', '<script', '<style',
            'window.location', 'document.cookie',
        }

        self.EXCLUDE_VALUES = {
            'false', 'undefined', 'delete', 'green', 'white', 'black',
            'gray', 'grey', 'color', 'home', 'index', 'login', 'logout',
            'register', 'signup', 'signin', 'user', 'admin', 'dashboard',
            'header', 'footer', 'sidebar', 'submit', 'reset', 'button',
            'input', 'form', 'https', 'localhost', 'base64', 'unicode',
        }

        self.SENSITIVE_KEYWORDS = {
            'key', 'secret', 'token', 'auth', 'password', 'pass', 'pwd',
            'credential', 'cert', 'api', 'access', 'private', 'private_key',
            'jwt', 'bearer', 'session', 'cookie', 'csrf', 'xsrf',
            'config', 'setting', 'env', 'environment',
        }

    def extract_candidates(self, js_code):
        """
        ä»JSä»£ç ä¸­æå–å¹¶è¿‡æ»¤å‡ºåˆæ ¼çš„å­—ç¬¦ä¸²åˆ—è¡¨
        è¿”å›: [(content, original_line), ...]
        """
        if not js_code:
            return []

        # ä½¿ç”¨æ›´æ˜ç¡®çš„å˜é‡åé¿å…å†²çª
        string_candidates = []
        lines = js_code.splitlines()
        quote_pattern = re.compile(r'(["\'])(.*?)\1')
        unicode_pattern = re.compile(r'(\\)+u[0-9a-fA-F]{4}')

        for line in lines:
            original_line = line.strip()
            if not original_line: continue
            if len(original_line) > 3500: continue
            if '"' not in original_line and "'" not in original_line: continue

            line_lower = original_line.lower()

            # æ£€æŸ¥ä¸Šä¸‹æ–‡å’Œå…³é”®è¯
            is_bad_context = any(ctx in line_lower for ctx in self.EXCLUDE_CONTEXTS)
            has_sensitive_keyword = any(kw in line_lower for kw in self.SENSITIVE_KEYWORDS)

            if is_bad_context and not has_sensitive_keyword:
                continue

            matches = quote_pattern.findall(original_line)

            for quote_char, content in matches:
                content = content.strip()
                content_len = len(content)

                if content_len > self.max_string_length:
                    continue

                min_len = self.min_sensitive_length if has_sensitive_keyword else self.min_string_length
                if content_len < min_len:
                    continue

                # å­—ç¬¦æ ¼å¼é™åˆ¶
                if ' ' in content or '<' in content or '>' in content or \
                        ':' in content or '\\' in content or '__' in content or \
                        '.' in content or '/' in content or '(' in content or ')' in content:
                    continue

                special_count = sum(1 for c in content if not c.isalnum())
                if special_count / content_len > 0.2:
                    continue

                # Unicode æ‹¦æˆª
                if len(unicode_pattern.findall(content)) >= 3:
                    continue
                if sum(1 for c in content if ord(c) > 127) > len(content) * 0.3:
                    continue

                if content in self.EXCLUDE_VALUES:
                    continue

                if content.isdigit() and content_len < 8:
                    continue

                if not has_sensitive_keyword:
                    if not any(char in original_line for char in ['=', ':', '{', '}']):
                        continue

                # åŒæ—¶ä¿å­˜å†…å®¹å’ŒåŸå§‹ä»£ç è¡Œ
                string_candidates.append((content, original_line))

        # å»é‡ï¼šä¿æŒé¡ºåºï¼Œå‰”é™¤é‡å¤çš„å€™é€‰å€¼
        string_candidates = list(set(string_candidates))
        return string_candidates


# ==================== ç¬¬äºŒæ­¥ï¼šç²¾è¿‡æ»¤å™¨ ====================
class AdvancedSecretFilter:
    def __init__(self, entropy_threshold=3.5, coverage_threshold=0.65):
        self.entropy_threshold = entropy_threshold
        self.coverage_threshold = coverage_threshold
        self.bloom_filter = ScalableBloomFilter(
    mode=ScalableBloomFilter.LARGE_SET_GROWTH,
    error_rate=0.01  # å…¨å±€è¯¯åˆ¤ç‡æ§åˆ¶åœ¨1%
)
        # åŠ è½½ NLTK è¯å…¸ä»¥å®ç° O(1) æŸ¥è¯¢
        self.english_vocab = set(w.lower() for w in words.words())
        self.COMMON_SHORT_WORDS = {
            'i', 'j', 'k', 'x', 'y', 'z', 'a', 'b', 'c', 'n', 'm', 't', 'p',
            'id', 'db', 'ip', 'to', 'in', 'on', 'up', 'at', 'by', 'of', 'if',
            'is', 'as', 'do', 'go', 'no', 'ok', 're', 'us', 'pi', 'io', 'ui', 'api',
            'ad', 'ae', 'ai', 'al', 'am', 'an', 'be', 'bi', 'bo', 'bu',
            'ca', 'co', 'cu', 'de', 'di', 'ed', 'el', 'em', 'en', 'es', 'ex',
            'fa', 'fi', 'fo', 'fu', 'ga', 'ge', 'gi', 'go', 'gu', 'ha', 'he',
            'hi', 'ho', 'hu', 'im', 'in', 'it', 'la', 'le', 'li', 'lo', 'lu',
            'ma', 'me', 'mi', 'mo', 'mu', 'na', 'ne', 'ni', 'no', 'nu', 'pa',
            'pe', 'pi', 'po', 'pu', 'ra', 're', 'ri', 'ro', 'ru', 'sa', 'se',
            'si', 'so', 'su', 'ta', 'te', 'ti', 'to', 'tu', 'un', 'us', 'va',
            've', 'vi', 'vo', 'vu', 'wa', 'we', 'wi', 'wo', 'wu', 'xa', 'xe',
            'xi', 'xo', 'xu', 'ya', 'ye', 'yi', 'yo', 'yu', 'za', 'ze', 'zi',
            'zo', 'zu'
        }

    def shannon_entropy(self, data):
        if not data:
            return 0
        counter = Counter(data)
        total_length = len(data)
        entropy = 0
        for char, count in counter.items():
            p_x = count / total_length
            if p_x > 0:
                entropy += -p_x * math.log(p_x, 2)
        return entropy

    def calculate_word_coverage(self, text):
        # è¿”å›ç»“æœè¶Šå¤§ï¼Œè¶Šè®¤ä¸ºæ˜¯éæ•æ„Ÿä¿¡æ¯
        if not text:
            return 1.0, []

        # ä½¿ç”¨bloom filter è¿‡æ»¤æ‰å·²ç»å‡ºç°è¿‡çš„å•è¯
        if text in self.bloom_filter:
            return 1.0, []
        # åŠ å…¥bloom filter
        self.bloom_filter.add(text)

        # 1. é¢„å¤„ç†ï¼šä¿ç•™å­—æ¯æ•°å­—å’Œä¸‹åˆ’çº¿
        clean_text = re.sub(r'[^a-zA-Z0-9-]', ' ', text)
        if not clean_text:
            return 0.0, []

        # 2. æ™ºèƒ½åˆ†è¯
        raw_words = wordninja.split(clean_text)

        weighted_score = 0
        valid_words = []

        for word in raw_words:
            word_lower = word.lower()
            word_length = len(word)

            if word_length >= 3:
                # éªŒè¯å•è¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„è‹±æ–‡å•è¯æˆ–ç¼–ç¨‹å¸¸ç”¨è¯
                is_valid_word = bool(wordnet.synsets(word_lower.lower()))

                if is_valid_word:
                    valid_words.append(word)

                    # é•¿å•è¯å¥–åŠ±é€»è¾‘ï¼šå•è¯è¶Šé•¿ï¼Œè¶Šä¸å¯èƒ½æ˜¯å¯†é’¥ç»„æˆéƒ¨åˆ†
                    # å¯†é’¥å“ˆå¸Œå¾ˆå°‘èƒ½æ‹†è§£å‡ºå¤šä¸ªæœ‰æ„ä¹‰çš„é•¿å•è¯
                    if word_length >= 5:
                        weighted_score += word_length * 2.0  # é•¿å•è¯ç»™äºˆ2.0å€æƒé‡
                    else:
                        weighted_score += word_length * 1.5  # ä¸­é•¿å•è¯ç»™äºˆ1.5å€æƒé‡
            else:
                # çŸ­è¯(<3å­—ç¬¦)å¤„ç†ï¼šé€šå¸¸æ˜¯å“ˆå¸Œç‰‡æ®µæˆ–æ— æ„ä¹‰å­—ç¬¦
                # ä¸ç»™äºˆæƒé‡åŠ åˆ†ï¼Œä½“ç°å…¶"æ··ä¹±æ€§"
                pass

        # 3. è®¡ç®—åŠ æƒè¦†ç›–ç‡
        ratio = weighted_score / len(clean_text) if len(clean_text) > 0 else 0

        # å½’ä¸€åŒ–å¤„ç†ï¼šé˜²æ­¢ratioè¶…è¿‡1.0ï¼ˆç”±äºé•¿å•è¯å¥–åŠ±ï¼‰
        final_ratio = min(ratio, 1.0)

        return final_ratio, valid_words

    def is_secret(self, text):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¯†é’¥"""
        if not text or len(text) < 4:
            return False

        code_syntax_indicators = ['${', '||', '&&', '?', '+=', '-=', '===', '!==', '?.', '??']
        if any(indicator in text for indicator in code_syntax_indicators):
            return False

        css_indicators = [
            # '-xs-', '-sm-', '-md-', '-lg-', '-xl-',  # å“åº”å¼æ–­ç‚¹
            # 'ml-', 'mr-', 'mt-', 'mb-', 'mx-', 'my-',  # Margin
            # 'pl-', 'pr-', 'pt-', 'pb-', 'px-', 'py-',  # Padding
            'bg-', 'text-', 'border-', 'font-',  # å¸¸è§å±æ€§
            'col-', 'row-', 'flex-', 'grid-' , # å¸ƒå±€
            'chunk-','data-' # è‡ªå®šä¹‰

        ]
        if any(ind in text for ind in css_indicators):
            return False, "CSS Class Pattern"

        entropy = self.shannon_entropy(text)

        digit_count = sum(c.isdigit() for c in text)
        digit_ratio = digit_count / len(text)

        if digit_ratio > 0.20:
            # 2. Hex ç‰¹å¾æ£€æŸ¥
            # å¦‚æœå­—ç¬¦ä¸²åªåŒ…å« hex å­—ç¬¦ä¸”é•¿åº¦è¾ƒé•¿ï¼Œç›´æ¥åˆ¤å®šä¸º Secretï¼Œè·³è¿‡åˆ†è¯æ£€æŸ¥
            if (len(text) == 16 or len(text) == 32) and re.match(r'^[0-9a-fA-F]+$', text):
                if entropy > 2.0:  # çº¯ Hex å¾ˆå®¹æ˜“è¢«åˆ¤å®šä¸ºå•è¯ï¼Œæ‰€ä»¥è¿™é‡Œå¼ºåˆ¶é€šè¿‡
                    return True, "Hex String Pattern"

        if entropy < self.entropy_threshold:
            return False

        coverage, _ = self.calculate_word_coverage(text)
        if coverage > self.coverage_threshold:
            return False

        sensitive_keywords = ['secret', 'key', 'token', 'password', 'auth', 'cred', 'cert']
        text_lower = text.lower()
        if any(keyword in text_lower for keyword in sensitive_keywords):
            if coverage > 0.4:
                return True

        return True


candidate_all = OrderedDict()
original_candidate_all = OrderedDict()

def remove_html_tags(html_text: str) -> str:
    """
    å»é™¤HTMLæ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬ï¼ˆå¤„ç†åµŒå¥—/å¸¦å±æ€§/è‡ªé—­åˆæ ‡ç­¾ï¼‰
    """
    # åˆ›å»ºè§£æå¯¹è±¡ï¼ˆæ¨èç”¨lxmlè§£æå™¨ï¼Œé€Ÿåº¦å¿«ï¼›æ— lxmlåˆ™ç”¨html.parserï¼‰
    soup = BeautifulSoup(html_text, "lxml")  # æˆ– "html.parser"
    # æå–æ‰€æœ‰çº¯æ–‡æœ¬ï¼ˆè‡ªåŠ¨å¿½ç•¥æ ‡ç­¾ï¼Œåˆå¹¶æ¢è¡Œ/ç©ºæ ¼ï¼‰
    pure_text = soup.get_text(strip=False)  # strip=False ä¿ç•™åŸæ¢è¡Œ/ç©ºæ ¼ï¼ŒTrueåˆ™å»é™¤é¦–å°¾ç©ºç™½
    return pure_text

def _limit_global_set_size(target_dict: OrderedDict, max_size: int):
    """âœ… æ–°å¢ç§æœ‰å‡½æ•°ï¼šå…¨å±€é›†åˆå®¹é‡æ§åˆ¶ï¼Œè¶…è¿‡ä¸Šé™è‡ªåŠ¨åˆ é™¤ã€æœ€æ—©æ’å…¥ã€‘çš„å…ƒç´ ï¼Œä¸»åŠ¨é‡Šæ”¾å†…å­˜"""
    if len(target_dict) > max_size:
        # è®¡ç®—éœ€è¦åˆ é™¤çš„å†—ä½™å…ƒç´ æ•°é‡ï¼Œå¤šåˆ 20%åšå†…å­˜é¢„ç•™
        del_count = len(target_dict) - max_size + int(max_size * 0.2)
        # æ‰¹é‡åˆ é™¤æœ€æ—©æ’å…¥çš„å…ƒç´  (OrderedDict.popitem(last=False) åˆ æœ€å‰é¢çš„å…ƒç´ )
        for _ in range(del_count):
            if target_dict:
                target_dict.popitem(last=False)
        # ä¸»åŠ¨è§¦å‘åƒåœ¾å›æ”¶ï¼Œç«‹åˆ»é‡Šæ”¾å†…å­˜ç¢ç‰‡
        # gc.collect()

class LLMSecretVerifier:
    def __init__(self, model_instance):
        self.llm = model_instance

    def _get_system_prompt(self):
        return (
            "Role: Binary Security Classifier.\n"
            "Objective: Identify hardcoded secrets.\n"
            "Instruction: For each ID, output 1 if it is a potential secret/key/token, output 0 if it is safe code/UI text.\n"
            "Policy: If unsure, output 1 (Recall > Precision).\n"
            "Output Format: Strict JSON object: `{\"id\": 1/0, ...}`"
        )

    def verify_candidates(self, candidates):
        if not candidates:
            return []

        # æ„é€ æå…¶ç´§å‡‘çš„è¾“å…¥ï¼Œåªç»™ ID å’Œå€¼
        input_data = {c['id']: c['value'] for c in candidates}
        formatted_input = json.dumps(input_data, ensure_ascii=False)

        messages = [
            SystemMessage(content=self._get_system_prompt()),
            HumanMessage(content=f"Classify these:\n{formatted_input}")
        ]

        try:
            response = self.llm.invoke(messages)
            content = response.content.strip()

            # æå– JSON å­—å…¸
            match = re.search(r'\{.*\}', content, re.DOTALL)
            json_str = match.group() if match else "{}"
            decision_dict = json.loads(json_str)

            verified_secrets = []
            for c in candidates:
                curr_id = str(c['id'])
                # å¦‚æœè¿”å› 1ï¼Œæˆ–è€…æ¨¡å‹æ²¡ç»™ç»“æœï¼Œæˆ–è€…æŠ¥é”™ï¼Œéƒ½ä¿ç•™ï¼ˆå®å¯é”™æ€ï¼‰
                decision = decision_dict.get(curr_id, 1)
                if str(decision) == "1":
                    verified_secrets.append(c)

            return verified_secrets

        except Exception as e:
            print(f"LLM Error: {e}. Keeping batch.")
            return candidates


def scan_js_code(js_code):
    """æ‰«æJSä»£ç ï¼Œè¿”å›æ•æ„Ÿä¿¡æ¯åˆ—è¡¨"""
    line_filter = CodeLineFilter()
    adv_filter = AdvancedSecretFilter()

    # æå–å€™é€‰å­—ç¬¦ä¸²ï¼ˆåŒæ—¶ä¿ç•™åŸå§‹ä»£ç è¡Œï¼‰
    candidates = line_filter.extract_candidates(js_code)

    results = []
    for content, original_line in candidates:
        if adv_filter.is_secret(content):
            coverage, _ = adv_filter.calculate_word_coverage(content)

            results.append({
                'secret': content,
                'line': original_line
            })

    return results

def load_ollama_llm():
    return ChatOllama(
        model=config.MODEL_NAME,
        temperature=config.MODEL_TEMPERATURE,
        max_tokens=config.MODEL_MAX_TOKENS,
        keep_alive=-1,
        reasoning=False
    )

def load_bailian_llm():
    """åŠ è½½é˜¿é‡Œäº‘ç™¾ç‚¼æ¨¡å‹ï¼ˆOpenAIå…¼å®¹æ¨¡å¼ï¼‰"""
    return ChatOpenAI(
        model=config.BAILIAN_MODEL_NAME,
        temperature=config.MODEL_TEMPERATURE,  # å¤ç”¨åŸæœ‰æ¸©åº¦é…ç½®ï¼Œåˆ†ç±»ä»»åŠ¡å¿…é¡»0
        max_tokens=config.MODEL_MAX_TOKENS,
        api_key=config.DASHSCOPE_API_KEY,
        base_url=config.DASHSCOPE_BASE_URL,
        stream=False,  # é‡ä¸­ä¹‹é‡ï¼šç»“æ„åŒ–JSONè¾“å‡ºå¿…é¡»å…³é—­æµå¼ï¼Œå¦åˆ™è§£æå¤±è´¥
        timeout=60     # è¶…æ—¶å…œåº•ï¼Œé˜²æ­¢äº‘ç«¯è¯·æ±‚å¡æ­»
    )


def load_llm_model():
    """
    è‡ªåŠ¨åˆ¤æ–­åŠ è½½å“ªä¸ªLLMæ¨¡å‹ï¼š
    1. å¦‚æœconfigä¸­æœ‰é˜¿é‡Œäº‘çš„æœ‰æ•ˆé…ç½® â†’ è¿”å›é˜¿é‡Œäº‘ç™¾ç‚¼æ¨¡å‹å®ä¾‹
    2. å¦åˆ™ â†’ è¿”å›æœ¬åœ°Ollamaæ¨¡å‹å®ä¾‹
    """
    # åˆ¤æ–­æ¡ä»¶ï¼šAPIå¯†é’¥ä¸ä¸ºç©º + åœ°å€ä¸ä¸ºç©º â†’ ç”¨é˜¿é‡Œäº‘
    if config.DASHSCOPE_API_KEY and config.DASHSCOPE_BASE_URL:
        print(f"\nğŸ”µ æ£€æµ‹åˆ°é˜¿é‡Œäº‘é…ç½®ï¼Œä½¿ç”¨ã€è¿œç¨‹APIã€‘æ¨¡å¼ - æ¨¡å‹: {config.BAILIAN_MODEL_NAME}")
        return load_bailian_llm()
    # å¦åˆ™ä½¿ç”¨æœ¬åœ°Ollama
    else:
        print(f"\nğŸŸ¢ æœªæ£€æµ‹åˆ°é˜¿é‡Œäº‘é…ç½®ï¼Œä½¿ç”¨ã€æœ¬åœ°Ollamaã€‘æ¨¡å¼ - æ¨¡å‹: {config.MODEL_NAME}")
        return load_ollama_llm()

def qwen_scan_js_code(js_code):
    # 1. é¢„å¤„ç†
    js_code = remove_html_tags(js_code)
    js_code = format_code(js_code, True)

    # 2. æå–å€™é€‰
    candidates = scan_js_code(js_code)

    if not candidates:
        return []

    # 3. è½¬æ¢ä¸ºå¯¹è±¡å¹¶å»é‡
    candidate_objects = []
    for i, candidate in enumerate(candidates):
        secret_val = candidate['secret']
        if secret_val in candidate_all:
            continue
        candidate_all[secret_val] = True
        _limit_global_set_size(candidate_all, config.MAX_CANDIDATE_ALL_SIZE)

        candidate_objects.append({
            "id": i,
            "value": secret_val,
            "original": candidate
        })

    if not candidate_objects:
        return []

    # éšæœºç†”æ–­
    MAX_LLM_CANDIDATES = 80  # ç¡¬é™åˆ¶ï¼šå•æ¬¡æœ€å¤šåªçœ‹ 80 ä¸ª

    if len(candidate_objects) > MAX_LLM_CANDIDATES:
        print(f"âš ï¸ è­¦å‘Šï¼šå‘ç° {len(candidate_objects)} ä¸ªå€™é€‰é¡¹ï¼Œè§¦å‘ç†”æ–­é™åˆ¶ã€‚")
        print(f"   æ­£åœ¨éšæœºé‡‡æ · {MAX_LLM_CANDIDATES} ä¸ªè¿›è¡Œæ£€æµ‹ï¼Œå…¶ä½™ä¸¢å¼ƒ...")

        # 1. éšæœºæ‰“ä¹±
        random.shuffle(candidate_objects)

        # 2. å¼ºåˆ¶æˆªæ–­
        candidate_objects = candidate_objects[:MAX_LLM_CANDIDATES]

        # 3. é‡ç½® ID
        for idx, obj in enumerate(candidate_objects):
            obj['id'] = idx

    print(f"ğŸš€ å‡†å¤‡å°† {len(candidate_objects)} ä¸ªå€™é€‰é€å…¥ LLM...")

    llm_model = load_llm_model()
    verifier = LLMSecretVerifier(llm_model)

    # æé«˜ batch_sizeï¼Œå› ä¸ºç°åœ¨å‰©ä¸‹çš„éƒ½æ˜¯ç²¾è‹±äº†ï¼Œæˆ–è€…æ•°é‡å·²ç»è¢«æˆ‘ä»¬é™åˆ¶ä½äº†
    batch_size = 30
    all_verified_results = []
    total_batches = (len(candidate_objects) + batch_size - 1) // batch_size

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
    for i in tqdm(range(0, len(candidate_objects), batch_size),
                  desc="ğŸ§  AI å®¡è®¡ä¸­",
                  total=total_batches,
                  unit="æ‰¹"):
        batch = candidate_objects[i: i + batch_size]
        batch_results = verifier.verify_candidates(batch)
        all_verified_results.extend(batch_results)

    # --- æœ€ç»ˆè¾“å‡ºï¼šè¿”å›æ¶‰åŠåˆ°æ•æ„Ÿä¿¡æ¯çš„æºä»£ç è¡Œ ---
    final_results = []
    for result in all_verified_results:
        original_line = result['original']['line']  # è·å–åŸå§‹è¡Œ

        # åŸºäºè¡Œå†…å®¹å»é‡ï¼Œé˜²æ­¢åŒä¸€è¡Œå‡ºç°å¤šä¸ª Key å¯¼è‡´é‡å¤è¾“å‡º
        if original_line in original_candidate_all:
            continue
        original_candidate_all[original_line] = True
        _limit_global_set_size(original_candidate_all, config.MAX_ORIGINAL_ALL_SIZE)
        final_results.append(original_line)

    return final_results