import asyncio
import json
import os
import time
import warnings

from tqdm import tqdm

from AI.SenInfo import qwen_scan_js_code

warnings.filterwarnings("ignore")
from colorama import init
from rich import print as rich_print
# from AI.Get_API import run_analysis
from FileIO.Excelrw import SafePathExcelGenerator
from HttpHandle.DuplicateChecker import DuplicateChecker
from HttpHandle.httpSend import get_source_async, fail_url
from JsHandle.pathScan import get_root_domain
from FileIO.filerw import write2json, clear_or_create_file, read
from parse_args import parse_args
from JsHandle.sensitiveInfoScan import find_all_info_by_rex


class Scanner:
    def __init__(self, args):
        self.args = args
        self.initial_urls = []  # åˆå§‹URL
        self.checker = None  # å»é‡ç®¡ç†å™¨ï¼ˆåç»­åˆå§‹åŒ–ï¼‰
        self.tmp_urls = set()  # ä¸´æ—¶URLåˆ—è¡¨
        self.whiteList = read("./config/whiteList")


    def run(self):
        """ä¸»è¿è¡Œé€»è¾‘"""
        os.makedirs("Result", exist_ok=True)
        # clear_or_create_file("Result/scanInfo.json")
        clear_or_create_file("Result/sensitiveInfo.json")

        self.initial_urls = self._load_initial_urls()
        if not self.initial_urls and not self.args.url:
            rich_print("[red]æœªæ‰¾åˆ°åˆå§‹URL[/red]")
            return

        self.checker = DuplicateChecker(initial_root_domain=self.initial_urls)
        # ä¸»è¦æ˜¯åœ¨httpsendæ¨¡å—ä½¿ç”¨
        self.args.initial_urls = self.initial_urls

        # å¼€å§‹æ‰«æ
        start_time = time.time()
        self._scan_recursive(self.load_url(self.args), 0)

        rich_print(f"[cyan]æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’[/cyan]")


    def _scan_recursive(self, urls, depth):
        """é€’å½’æ‰«æï¼ˆæŒ‰æ·±åº¦è¿­ä»£ï¼‰"""
        if depth > self.args.height:
            return

        # è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œç¡®ä¿é¡ºåº
        urls_list = list(urls) if isinstance(urls, set) else urls

        # æ¸…ç†URLç©ºæ ¼
        urls_list = [url.strip() for url in urls_list if url.strip()]

        print(f"[bold green]ğŸ” æ·±åº¦ {depth} æ‰«æå¼€å§‹ï¼ŒURLæ€»æ•°: {len(urls_list)}[/bold green]")

        # åˆ†æ‰¹æ¬¡å¤„ç†ï¼ˆæ¯æ‰¹1000ä¸ªï¼‰
        batch_size = 1000
        total_batches = (len(urls_list) + batch_size - 1) // batch_size

        # å­˜å‚¨æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœï¼ˆåªç”¨äºé€’å½’å’Œæ•æ„Ÿä¿¡æ¯æå–ï¼‰
        all_scan_info_list = []
        all_next_urls = set()

        for batch_idx in range(0, len(urls_list), batch_size):
            batch_urls = urls_list[batch_idx:batch_idx + batch_size]
            current_batch = batch_idx // batch_size + 1

            print(
                f"\n[bold cyan]ğŸ“¦ æ·±åº¦ {depth} - URLæ‰«ææ‰¹æ¬¡ {current_batch}/{total_batches} (URLæ•°é‡: {len(batch_urls)})[/bold cyan]")

            # è°ƒç”¨get_source_asyncå¤„ç†å½“å‰æ‰¹æ¬¡
            batch_all_next_urls_with_source, batch_scan_info_list, batch_next_urls = asyncio.run(
                get_source_async(
                    urls=batch_urls,
                    thread_num=self.args.thread_num,
                    args=self.args,
                    checker=self.checker
                )
            )

            # å»é‡å¤„ç†
            batch_next_urls = batch_next_urls - self.tmp_urls
            if batch_next_urls:
                self.tmp_urls |= batch_next_urls

            # ç»Ÿè®¡å½“å‰æ‰¹æ¬¡çš„URLæ•°é‡
            current_batch_urls_count = 0
            for item in batch_all_next_urls_with_source:
                if isinstance(item, dict) and "next_urls" in item:
                    next_urls = item["next_urls"]
                    if isinstance(next_urls, (list, set, tuple)):
                        current_batch_urls_count += len(next_urls)

            # ç«‹å³å°†å½“å‰æ‰¹æ¬¡ç»“æœå†™å…¥Excel
            if batch_all_next_urls_with_source:
                print(f"[bold blue]ğŸ’¾ ç«‹å³å†™å…¥æ·±åº¦ {depth} - æ‰¹æ¬¡ {current_batch} çš„æ•°æ®åˆ°Excel "
                      f"({len(batch_all_next_urls_with_source)} ä¸ªæ‰¹æ¬¡æ¡ç›®ï¼Œçº¦ {current_batch_urls_count} ä¸ªURL)[/bold blue]")

                try:
                    excel_handler.append_data_batch(
                        input_data=batch_all_next_urls_with_source,
                        batch_size=1000,
                        show_progress=False  # é¿å…åµŒå¥—è¿›åº¦æ¡
                    )
                    print(f"[green]âœ… æ·±åº¦ {depth} - æ‰¹æ¬¡ {current_batch} æ•°æ®å†™å…¥ExcelæˆåŠŸ[/green]")
                except Exception as e:
                    print(f"[red]âŒ æ·±åº¦ {depth} - æ‰¹æ¬¡ {current_batch} æ•°æ®å†™å…¥Excelå¤±è´¥: {str(e)}[/red]")

            # åˆå¹¶ç»“æœç”¨äºåç»­å¤„ç†
            all_scan_info_list.extend(batch_scan_info_list)
            all_next_urls.update(batch_next_urls)

            # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œé‡Šæ”¾èµ„æº
            if current_batch < total_batches:
                print(f"[yellow]â³ æ·±åº¦ {depth} - URLæ‰«ææ‰¹æ¬¡ {current_batch} å®Œæˆï¼Œç­‰å¾… 1 ç§’é‡Šæ”¾èµ„æº...[/yellow]")
                time.sleep(1)

        # å¤„ç†æ•æ„Ÿä¿¡æ¯ï¼ˆæ‰€æœ‰æ‰¹æ¬¡å®Œæˆåç»Ÿä¸€å¤„ç†ï¼‰
        if self.args.sensitiveInfo or self.args.sensitiveInfoQwen:
            print(f"[bold magenta]ğŸ” å¼€å§‹æ•æ„Ÿä¿¡æ¯æå–ï¼Œæ€»æ•°æ®é‡: {len(all_scan_info_list)}[/bold magenta]")
            self._extract_sensitive_info(all_scan_info_list)

        # é€’å½’ä¸‹ä¸€å±‚
        if all_next_urls:
            print(
                f"[bold blue]â¡ï¸  æ·±åº¦ {depth} å®Œæˆï¼Œå‘ç° {len(all_next_urls)} ä¸ªæ–°URLï¼Œè¿›å…¥æ·±åº¦ {depth + 1}[/bold blue]")
            self._scan_recursive(all_next_urls, depth + 1)
        else:
            print(f"[bold green]âœ… æ·±åº¦ {depth} å®Œæˆï¼Œæœªå‘ç°æ–°URL[/bold green]")

    def _extract_sensitive_info(self, scan_info_list):
        """æå–æ•æ„Ÿä¿¡æ¯ï¼ˆä»æœ‰æ•ˆæ‰«æç»“æœä¸­ï¼‰"""
        sensitive_info = []
        for scan_info in scan_info_list:
            url = scan_info["url"]
            if scan_info["is_valid"] == 1 or url in self.initial_urls:
                if ".js" not in scan_info["url"]:
                    continue
                if args.sensitiveInfoQwen:
                    sensitive_info = qwen_scan_js_code(scan_info["source_code"])
                elif args.sensitiveInfo:
                    sensitive_info = find_all_info_by_rex(scan_info["source_code"])
                if len(sensitive_info) == 0:
                    print(f"URL: {url} æ²¡æœ‰æ•æ„Ÿä¿¡æ¯")
                    continue
                write2json(
                    "Result/sensitiveInfo.json",
                    json.dumps(
                        {"url": url, "sensitive_info": sensitive_info},
                        indent=4,  # åŠ ç¼©è¿›ï¼Œç”Ÿæˆæ ¼å¼åŒ–JSONå­—ç¬¦ä¸²
                        ensure_ascii=False  # é¿å…éASCIIå­—ç¬¦è½¬ä¹‰ï¼ˆå’Œwriteré‡Œçš„å‚æ•°å¯¹é½ï¼‰
                    )
                )
                rich_print(
                    f"[bold orange]URL:[/bold orange] {url}\n"
                    f"\t[bold orange]æ•æ„Ÿä¿¡æ¯:[/bold orange] {sensitive_info}"
                )

    def load_url(self,args):
        if args.url is not None:
            return [args.url]
        if args.batch is not None:
            return read(args.batch)
        return []

    def _load_initial_urls(self):
        """åŠ è½½åˆå§‹URLï¼ˆä»æ‰¹é‡æ–‡ä»¶æˆ–å‚æ•°ï¼‰"""
        if self.args.batch:
            from JsHandle.pathScan import read
            domains = read("./config/whiteList")
            if len(domains) == 0:
                domains = [get_root_domain(url) for url in read(self.args.batch)]
            return domains
        elif self.args.url:
            return [get_root_domain(self.args.url)]
        return []

if __name__ == '__main__':
    init(autoreset=True)
    args = parse_args()
    # load whiteList
    start_time = time.time()
    excel_handler = SafePathExcelGenerator('Result/Result.xlsx')
    scanner = Scanner(args)
    scanner.run()
    rich_print(f"[bold]è¯·æ±‚å¤±è´¥çš„urlï¼š[/bold][underline]{str(fail_url)}[/underline]")
    rich_print(f"[bold]è€—æ—¶ï¼š{time.time() - start_time}[/bold]")
