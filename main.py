import asyncio
import json
import os
import time
import warnings

import requests
from tqdm import tqdm

from AI.SenInfo import qwen_scan_js_code

warnings.filterwarnings("ignore")
from colorama import init
from rich import print as rich_print
# from AI.Get_API import run_analysis
from FileIO.Excelrw import SafePathExcelGenerator
from HttpHandle.DuplicateChecker import DuplicateChecker
from HttpHandle.httpSend import get_source_async, fail_url
from JsHandle.pathScan import get_root_domain
from FileIO.filerw import write2json, clear_or_create_file, read
from parse_args import parse_args
from JsHandle.sensitiveInfoScan import find_all_info_by_rex


class Scanner:
    def __init__(self, args):
        self.args = args
        self.initial_urls = []  # åˆå§‹URLæ ¹åŸŸï¼ˆç™½åå•+ä¼ å…¥URLæ ¹åŸŸï¼‰
        self.checker = None  # å»é‡ç®¡ç†å™¨ï¼ˆåç»­åˆå§‹åŒ–ï¼‰
        self.tmp_urls = set()  # ä¸´æ—¶URLåˆ—è¡¨
        self.whiteList = read("./config/whiteList")  # ä¿ç•™åŸç™½åå•è¯»å–ï¼Œæœªæ”¹åŠ¨

    def run(self):
        """ä¸»è¿è¡Œé€»è¾‘"""
        os.makedirs("Result", exist_ok=True)
        clear_or_create_file("Result/sensitiveInfo.json")

        self.initial_urls = self._load_initial_urls()
        scan_seed_urls = self.load_url(self.args)

        if not scan_seed_urls:
            rich_print("[red]æœªä¼ å…¥è¦æ‰«æçš„åˆå§‹URLï¼Œè¯·æŒ‡å®š--urlå‚æ•°[/red]")
            return

        self.checker = DuplicateChecker(initial_root_domain=self.initial_urls)
        # ä¸»è¦æ˜¯åœ¨httpsendæ¨¡å—ä½¿ç”¨
        self.args.initial_urls = self.initial_urls

        # å¼€å§‹æ‰«æ
        start_time = time.time()
        self._scan_recursive(self.load_url(self.args), 0)

        rich_print(f"[cyan]æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’[/cyan]")

    def load_url(self, args):
        if args.url and args.url.strip():
            return [args.url.strip()]
        return []

    def _load_initial_urls(self):
        # ç¬¬ä¸€æ­¥ï¼šå¼ºåˆ¶åŠ è½½ç™½åå•ï¼Œæ— æ¡ä»¶å¿…åŠ è½½
        white_list_domains = read("./config/whiteList")
        # ç¬¬äºŒæ­¥ï¼šå¦‚æœä¼ å…¥äº†æ‰«æURLï¼Œè§£ææ ¹åŸŸåå¹¶è¿½åŠ 
        if self.args.url and self.args.url.strip():
            try:
                seed_root_domain = get_root_domain(self.args.url.strip())
                if seed_root_domain and seed_root_domain not in white_list_domains:
                    white_list_domains.append(seed_root_domain)
            except Exception:
                pass
        # å»é‡+è¿‡æ»¤ç©ºå­—ç¬¦ä¸²ï¼Œè¿”å›æœ€ç»ˆæ ¡éªŒç”¨çš„æ ¹åŸŸååˆ—è¡¨
        return list(set(filter(None, white_list_domains)))

    def _scan_recursive(self, urls, depth):
        """é€’å½’æ‰«æï¼ˆæŒ‰æ·±åº¦è¿­ä»£ï¼‰"""
        if depth > self.args.height:
            return

        # è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œç¡®ä¿é¡ºåº
        urls_list = list(urls) if isinstance(urls, set) else urls

        # æ¸…ç†URLç©ºæ ¼
        urls_list = [url.strip() for url in urls_list if url.strip()]

        print(f"[bold green]ğŸ” æ·±åº¦ {depth} æ‰«æå¼€å§‹ï¼ŒURLæ€»æ•°: {len(urls_list)}[/bold green]")

        # åˆ†æ‰¹æ¬¡å¤„ç†ï¼ˆæ¯æ‰¹1000ä¸ªï¼‰
        batch_size = 500
        total_batches = (len(urls_list) + batch_size - 1) // batch_size

        # å­˜å‚¨æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœï¼ˆåªç”¨äºé€’å½’å’Œæ•æ„Ÿä¿¡æ¯æå–ï¼‰
        all_scan_info_list = []
        all_next_urls = set()

        for batch_idx in range(0, len(urls_list), batch_size):
            batch_urls = urls_list[batch_idx:batch_idx + batch_size]
            current_batch = batch_idx // batch_size + 1

            print(
                f"\n[bold cyan]ğŸ“¦ æ·±åº¦ {depth} - URLæ‰«ææ‰¹æ¬¡ {current_batch}/{total_batches} (URLæ•°é‡: {len(batch_urls)})[/bold cyan]")

            # è°ƒç”¨get_source_asyncå¤„ç†å½“å‰æ‰¹æ¬¡
            batch_all_next_urls_with_source, batch_scan_info_list, batch_next_urls = asyncio.run(
                get_source_async(
                    urls=batch_urls,
                    thread_num=self.args.thread_num,
                    args=self.args,
                    checker=self.checker
                )
            )

            # å»é‡å¤„ç†
            batch_next_urls = batch_next_urls - self.tmp_urls
            if batch_next_urls:
                self.tmp_urls |= batch_next_urls

            # ç»Ÿè®¡å½“å‰æ‰¹æ¬¡çš„URLæ•°é‡
            current_batch_urls_count = 0
            for item in batch_all_next_urls_with_source:
                if isinstance(item, dict) and "next_urls" in item:
                    next_urls = item["next_urls"]
                    if isinstance(next_urls, (list, set, tuple)):
                        current_batch_urls_count += len(next_urls)

            # ç«‹å³å°†å½“å‰æ‰¹æ¬¡ç»“æœå†™å…¥Excel
            if batch_all_next_urls_with_source:
                print(f"[bold blue]ğŸ’¾ ç«‹å³å†™å…¥æ·±åº¦ {depth} - æ‰¹æ¬¡ {current_batch} çš„æ•°æ®åˆ°Excel "
                      f"({len(batch_all_next_urls_with_source)} ä¸ªæ‰¹æ¬¡æ¡ç›®ï¼Œçº¦ {current_batch_urls_count} ä¸ªURL)[/bold blue]")

                try:
                    excel_handler.append_data_batch(
                        input_data=batch_all_next_urls_with_source,
                        batch_size=500,
                        show_progress=False  # é¿å…åµŒå¥—è¿›åº¦æ¡
                    )
                    print(f"[green]âœ… æ·±åº¦ {depth} - æ‰¹æ¬¡ {current_batch} æ•°æ®å†™å…¥ExcelæˆåŠŸ[/green]")
                except Exception as e:
                    print(f"[red]âŒ æ·±åº¦ {depth} - æ‰¹æ¬¡ {current_batch} æ•°æ®å†™å…¥Excelå¤±è´¥: {str(e)}[/red]")

            # åˆå¹¶ç»“æœç”¨äºåç»­å¤„ç†
            all_scan_info_list.extend(batch_scan_info_list)
            all_next_urls.update(batch_next_urls)

            # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œé‡Šæ”¾èµ„æº
            if current_batch < total_batches:
                print(f"[yellow]â³ æ·±åº¦ {depth} - URLæ‰«ææ‰¹æ¬¡ {current_batch} å®Œæˆï¼Œç­‰å¾… 1 ç§’é‡Šæ”¾èµ„æº...[/yellow]")
                time.sleep(1)

        # å¤„ç†æ•æ„Ÿä¿¡æ¯ï¼ˆæ‰€æœ‰æ‰¹æ¬¡å®Œæˆåç»Ÿä¸€å¤„ç†ï¼‰
        if self.args.sensitiveInfo or self.args.sensitiveInfoQwen:
            print(f"[bold magenta]ğŸ” å¼€å§‹æ•æ„Ÿä¿¡æ¯æå–ï¼Œæ€»æ•°æ®é‡: {len(all_scan_info_list)}[/bold magenta]")
            self._extract_sensitive_info(all_scan_info_list)

        # é€’å½’ä¸‹ä¸€å±‚
        if all_next_urls:
            print(
                f"[bold blue]â¡ï¸  æ·±åº¦ {depth} å®Œæˆï¼Œå‘ç° {len(all_next_urls)} ä¸ªæ–°URLï¼Œè¿›å…¥æ·±åº¦ {depth + 1}[/bold blue]")
            self._scan_recursive(all_next_urls, depth + 1)
        else:
            print(f"[bold green]âœ… æ·±åº¦ {depth} å®Œæˆï¼Œæœªå‘ç°æ–°URL[/bold green]")

    def _extract_sensitive_info(self, scan_info_list):
        """æå–æ•æ„Ÿä¿¡æ¯ï¼ˆä»æœ‰æ•ˆæ‰«æç»“æœä¸­ï¼‰"""
        sensitive_info = []
        for scan_info in scan_info_list:
            url = scan_info["url"]
            if scan_info["is_valid"] == 1 or url in self.initial_urls:
                if ".js" not in scan_info["url"]:
                    continue
                # ========== ã€ä¿®å¤è‡´å‘½è¯­æ³•é”™è¯¯ï¼šargs â†’ self.args å¦åˆ™è¿è¡Œå¿…æŠ¥é”™ã€‘ ==========
                if self.args.sensitiveInfoQwen:
                    sensitive_info = qwen_scan_js_code(scan_info["source_code"])
                elif self.args.sensitiveInfo:
                    sensitive_info = find_all_info_by_rex(scan_info["source_code"])
                if len(sensitive_info) == 0:
                    print(f"URL: {url} æ²¡æœ‰æ•æ„Ÿä¿¡æ¯")
                    continue
                write2json(
                    "Result/sensitiveInfo.json",
                    json.dumps(
                        {"url": url, "sensitive_info": sensitive_info},
                        indent=4,  # åŠ ç¼©è¿›ï¼Œç”Ÿæˆæ ¼å¼åŒ–JSONå­—ç¬¦ä¸²
                        ensure_ascii=False  # é¿å…éASCIIå­—ç¬¦è½¬ä¹‰ï¼ˆå’Œwriteré‡Œçš„å‚æ•°å¯¹é½ï¼‰
                    )
                )
                rich_print(
                    f"[bold orange]URL:[/bold orange] {url}\n"
                    f"\t[bold orange]æ•æ„Ÿä¿¡æ¯:[/bold orange] {sensitive_info}"
                )

FEISHU_WEBHOOK = "https://open.feishu.cn/open-apis/bot/v2/hook/xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"


def send_feishu_notify(title, content=""):
    """é£ä¹¦æ¨é€ã€çº¯æ–‡æœ¬ä¸‡èƒ½ç‰ˆã€‘- 100%å…¼å®¹æ‰€æœ‰é£ä¹¦æœºå™¨äººï¼Œå½»åº•è§£å†³10208é”™è¯¯"""
    if not FEISHU_WEBHOOK or "ä½ çš„æ­£ç¡®é£ä¹¦åœ°å€" in FEISHU_WEBHOOK:
        rich_print("[red][bold]âš ï¸ æœªé…ç½®æ­£ç¡®çš„é£ä¹¦Webhookåœ°å€ï¼Œè·³è¿‡æ¨é€[/bold][/red]")
        return
    try:
        send_data = {
            "msg_type": "text",  # å¿…é¡»æ˜¯textï¼Œä¸èƒ½æ˜¯markdown
            "content": {
                "text": f"{title}\n{content}"  # \n å°±æ˜¯æ¢è¡Œï¼Œæ’ç‰ˆå’Œä¹‹å‰ä¸€æ ·æ¸…æ™°
            }
        }
        headers = {"Content-Type": "application/json; charset=utf-8"}
        res = requests.post(FEISHU_WEBHOOK, json=send_data, headers=headers, timeout=10)
        res_json = res.json()
        # é£ä¹¦çº¯æ–‡æœ¬æ¨é€æˆåŠŸçš„è¿”å›ç æ˜¯ 0
        if res_json.get("StatusCode") == 0:
            rich_print("[green][bold]âœ… é£ä¹¦æ¶ˆæ¯æ¨é€æˆåŠŸ âœ…[/bold][/green]")
        else:
            rich_print(f"[red][bold]âŒ é£ä¹¦æ¨é€å¤±è´¥: {res.text}[/bold][/red]")
    except Exception as e:
        rich_print(f"[yellow][bold]âš ï¸ é£ä¹¦æ¨é€æ¥å£å¼‚å¸¸: {str(e)}[/bold][/yellow]")


if __name__ == '__main__':
    init(autoreset=True)
    args = parse_args()

    start_time = time.time()
    # 1. ç¡®ä¿Resultæ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs("Result", exist_ok=True)
    # 2. è·å–æ‰«æçš„ç›®æ ‡URLå¹¶æå–æ ¹åŸŸå
    target_url = args.url.strip() if args.url else "unknown_url"
    try:
        url_domain = get_root_domain(target_url)
    except:
        url_domain = "unknown_domain"
    # 3. æ ¼å¼åŒ–åŸŸåï¼šæ›¿æ¢ç‰¹æ®Šå­—ç¬¦ï¼Œå…¼å®¹Windows/Linuxæ–‡ä»¶åè§„åˆ™
    format_domain = url_domain.replace(".", "_").replace("/", "_").replace(":", "_")
    # 4. ç”Ÿæˆç²¾ç¡®æ—¶é—´æˆ³ï¼šå¹´æœˆæ—¥_æ—¶åˆ†ç§’ (æ— éæ³•å­—ç¬¦ï¼Œæ’åºå‹å¥½)
    time_str = time.strftime("%Y%m%d_%H%M%S", time.localtime())
    # 5. æ‹¼æ¥æœ€ç»ˆExcelæ–‡ä»¶åï¼šResult/Result_åŸŸå_æ—¶é—´.xlsx
    excel_filename = f"Result/Result_{format_domain}_{time_str}.xlsx"
    # 6. å®ä¾‹åŒ–Excelå¤„ç†å™¨
    excel_handler = SafePathExcelGenerator(excel_filename)
    scanner = Scanner(args)

    try:
        # æ‰§è¡Œæ ¸å¿ƒæ‰«æé€»è¾‘
        scanner.run()

        run_time = round(time.time() - start_time, 2)
        # é£ä¹¦æ¨é€çš„å†…å®¹ï¼ŒåŒ…å«ä½ éœ€è¦çš„ã€å¤±è´¥urlã€‘+ã€è€—æ—¶ã€‘+è¿è¡Œç»“æœ
        notify_content = f"""
âœ… **ç¨‹åºè¿è¡Œå®Œæˆï¼æ‰«æä»»åŠ¡ç»“æŸ**
ğŸ“Š è¿è¡Œè€—æ—¶ï¼š{run_time} ç§’
ğŸ“„ ç»“æœæ–‡ä»¶ï¼šResult/Result.xlsx
ğŸ•’ å®Œæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}
"""
        send_feishu_notify("ã€æœåŠ¡å™¨-æ‰«æä»»åŠ¡âœ…æ‰§è¡Œå®Œæˆã€‘", notify_content)

        rich_print(f"[bold]è€—æ—¶ï¼š{run_time}[/bold]")

    except Exception as e:
        # æ•è·æ‰€æœ‰æŠ¥é”™ï¼Œè·å–å®Œæ•´æŠ¥é”™å †æ ˆä¿¡æ¯
        run_time = round(time.time() - start_time, 2)
        # æŠ¥é”™çš„é£ä¹¦æŠ¥è­¦å†…å®¹ï¼Œé†’ç›®çº¢è‰²æé†’
        error_content = f"""
            âŒ **ç¨‹åºè¿è¡Œå‡ºé”™ï¼æ‰«æä»»åŠ¡ç»ˆæ­¢**
            âš ï¸ é”™è¯¯ç±»å‹ï¼š{type(e).__name__}
            âš ï¸ é”™è¯¯è¯¦æƒ…ï¼š{str(e)}
            â±ï¸ è¿è¡Œè€—æ—¶ï¼š{run_time} ç§’
            ğŸ•’ æŠ¥é”™æ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}

            """
        send_feishu_notify("ã€æœåŠ¡å™¨-æ‰«æä»»åŠ¡âŒå´©æºƒæŠ¥è­¦ã€‘", error_content)
