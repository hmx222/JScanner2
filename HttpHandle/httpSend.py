import asyncio
from contextlib import asynccontextmanager
from urllib.parse import urlparse, urljoin

import requests
from playwright.async_api import Request
from urllib3.exceptions import InsecureRequestWarning

requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
import requests
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page, Browser, BrowserContext
from rich import print
from rich.markup import escape
from tqdm.asyncio import tqdm_asyncio
from urllib3.exceptions import InsecureRequestWarning
from user_agent import generate_user_agent

from HttpHandle.DuplicateChecker import DuplicateChecker
from JsHandle.pathScan import get_root_domain
from parse_args import parse_headers

requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
# å®šä¹‰ä¸éœ€è¦åŠ è½½çš„èµ„æºç±»å‹ï¼ŒèŠ‚çœå¸¦å®½å’Œæ¸²æŸ“æ—¶é—´
BLOCKED_RESOURCE_TYPES = {"image", "media", "font", "stylesheet"}

def fix_encoding(text):
    """å°è¯•ä¿®å¤ä¹±ç å­—ç¬¦ä¸²"""
    encodings = ['utf-8', 'gbk', 'gb2312', 'latin1', 'iso-8859-1']

    for enc in encodings:
        try:
            # å…ˆç”¨å½“å‰ç¼–ç ç¼–ç ï¼Œå†ç”¨utf-8è§£ç 
            return text.encode(enc).decode('utf-8')
        except (UnicodeEncodeError, UnicodeDecodeError):
            continue

    # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
    return text

# âœ… ä¿®æ”¹ç‚¹1ï¼šã€æ ¸å¿ƒã€‘æ¥æ”¶å…¨å±€å”¯ä¸€Contextï¼Œåªåˆ›å»ºPage(Tab)ï¼Œç”¨å®Œä»…å…³é—­Tabï¼ŒContextå…¨å±€å¤ç”¨
# æ•ˆæœï¼šä¸€ä¸ªæµè§ˆå™¨çª—å£å†…çš„å¤šä¸ªTabï¼Œ10çº¿ç¨‹=10ä¸ªTabï¼Œå®Œå…¨ç‹¬ç«‹ï¼Œæ— è¿›ç¨‹æ³„æ¼
@asynccontextmanager
async def get_playwright_page(context: BrowserContext):
    """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šåˆ›å»ºå’Œè‡ªåŠ¨å…³é—­é¡µé¢ã€ä»…åˆ›å»ºTabï¼Œå…¨å±€ä¸€ä¸ªæµè§ˆå™¨ç¯å¢ƒã€‘"""
    page = await context.new_page()
    try:
        yield page
    finally:
        # æ ¸å¿ƒä¿®å¤ï¼šç»™page.close()å¢åŠ ã€è¶…æ—¶å¼ºåˆ¶å…œåº•ã€‘ï¼Œç ´è§£å¼‚æ­¥æ­»é”æ°¸ä¸è¿”å›çš„é—®é¢˜
        try:
            await asyncio.wait_for(page.close(), timeout=3.0)
        except asyncio.TimeoutError:
            pass  # è¶…æ—¶å°±æ”¾å¼ƒï¼Œä¸æŠ¥é”™ã€ä¸é˜»å¡ã€ç¨‹åºç»§ç»­èµ°

fail_url = set()

async def fetch_page_async(page: Page, url: str, progress: tqdm_asyncio, headers_: dict):
    """
    ä¿æŒåŸæœ‰è¿”å›å€¼ç»“æ„: (html_content, url, status)
    ä½†é€šè¿‡ç›‘å¬ï¼Œç¡®ä¿ html_content åŒ…å«äº†æ‰€æœ‰åŠ¨æ€å‘ç°çš„ JS
    """
    discovered_js = set()
    handle_request = None
    try:
        # 1. æ‹¦æˆªæ— å…³èµ„æºï¼ˆæé€Ÿæ ¸å¿ƒï¼‰
        await page.route("**/*", lambda route: route.abort()
        if route.request.resource_type in BLOCKED_RESOURCE_TYPES
        else route.continue_())

        # 2. ç›‘å¬æ‰€æœ‰ JS è¯·æ±‚
        def handle_request(request: Request):
            if request.resource_type == "script" or request.url.split('?')[0].endswith('.js'):
                discovered_js.add(request.url)

        page.on("request", handle_request)

        if headers_:
            await page.set_extra_http_headers(parse_headers(headers_))

        # 3. å¯¼èˆª
        response = await page.goto(url, wait_until="domcontentloaded", timeout=15000)
        status = response.status if response else 500

        # 4. è·å–å½“å‰é¡µé¢çš„ HTML
        html_content = await page.content()

        if discovered_js:
            extra_scripts = "".join([f'<script src="{escape(js)}"></script>' for js in discovered_js])
            html_content = html_content.replace("</body>", f"{extra_scripts}</body>")

        return html_content, url, status

    except Exception:
        fail_url.add(url)
        return None, url, None
    finally:
        # âœ… è¡¥å……ï¼šç§»é™¤ç›‘å¬ï¼Œæ— å†…å­˜æ³„æ¼ï¼Œä¸å½±å“ä½ çš„é€»è¾‘
        if handle_request:
            page.remove_listener("request", handle_request)
        progress.update(1)

async def process_scan_result(scan_info, checker: DuplicateChecker, args):
    """å¤„ç†æ‰«æç»“æœï¼ˆå»é‡+æå–ä¸‹ä¸€å±‚URLï¼‰"""
    url = scan_info["url"]
    source = scan_info["source_code"]
    status = scan_info["status"]
    title = scan_info["title"]
    length = scan_info["length"]

    # åŸºç¡€è¿‡æ»¤ï¼ˆæ— æ•ˆURL/é”™è¯¯çŠ¶æ€ç /è¿‡çŸ­æºç ï¼‰
    if not checker.is_valid_url(url):
        return False, set()
    if status and status >= 404:
        return False, set()
    if not source or length < 200:
        return False, set()

    if ".js" not in url:
        # å»é‡æ£€æŸ¥ï¼ˆæŒ‰é…ç½®çš„ç­–ç•¥æ‰§è¡Œï¼‰
        if (args.de_duplication_hash and checker.check_duplicate_by_DOM_simhash(source,args.de_duplication_hash)) or \
                (args.de_duplication_title and checker.check_duplicate_by_title(title, url)) or \
                (args.de_duplication_length and checker.check_duplicate_by_length(length, url)) or \
                (args.de_duplication_similarity and checker.check_duplicate_by_simhash(
                    source, url, float(args.de_duplication_similarity))):
            return False, set()

    checker.mark_url_visited(url)

    # æå–ä¸‹ä¸€å±‚URLï¼ˆä»…JSæ–‡ä»¶æˆ–åˆå§‹URLéœ€è¦ï¼‰
    next_urls = set()
    all_dirty = []
    if ".js" in url or get_root_domain(url) in args.initial_urls:
        from JsHandle.pathScan import analysis_by_rex, data_clean

        if not args.ollama:
            all_dirty = analysis_by_rex(source)
        else:
            rex_output = analysis_by_rex(source)
            all_dirty.extend(rex_output)
            # if is_js_file(url) and not source.startswith("<!DOCTYPE html>") and len(source) > 1000 and len(rex_output) >= 6 :
            #     try:
            #         print("ğŸ¤” å¤§æ¨¡å‹æ­£åœ¨åˆ†æä¸­ ğŸ”ğŸ’¡")
            #         source = extract_pure_js(source)
            #         ollama_output = clean_output(run_analysis(source))
            #         all_dirty.extend(ollama_output)
            #     except:
            #         print(
            #             f"[bold]å½“å‰å¤„ç†çš„URL:[/bold]\n"
            #             f"  [blue underline]{url}[/blue underline]\n"
            #             f"[orange]âš ï¸ ç¾åŒ–JavaScriptæ—¶å¯èƒ½å‡ºç°é”™è¯¯[/orange]\n"
            #             f"[green]â†’ ç»§ç»­æ‰§è¡Œæ­£å¸¸ä»»åŠ¡[/green]"
            #         )

        next_urls = set(data_clean(url, all_dirty))

    return True, next_urls

def get_webpage_title(html_source):
    """
    get webpage title
    """
    soup = BeautifulSoup(html_source, 'html.parser')
    title_tag = soup.find('title')
    if title_tag:
        return title_tag.text
    return "NULL"

async def get_source_async(urls, thread_num, args, checker: DuplicateChecker):
    """Playwrightå¼‚æ­¥æ‰¹é‡è¯·æ±‚+å»é‡å¤„ç†å…¥å£"""

    progress = tqdm_asyncio(total=len(urls), desc="Process", unit="url", ncols=100)

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=not args.visible,
            proxy={"server": args.proxy} if args.proxy else None,
            args=["--disable-gpu", "--no-sandbox", "--disable-dev-shm-usage"] # âœ… è¡¥å…¨Linuxå¿…åŠ å‚æ•°ï¼Œè§£å†³å†…å­˜ä¸è¶³ï¼Œæ— å‰¯ä½œç”¨
        )
        # âœ… ä¿®æ”¹ç‚¹2ï¼šã€æ ¸å¿ƒã€‘å…¨å±€åªåˆ›å»º1ä¸ªContext â†’ å¯¹åº”1ä¸ªæµè§ˆå™¨çª—å£ï¼Œæ‰€æœ‰Tabéƒ½åœ¨è¿™ä¸ªçª—å£å†…
        global_context = await browser.new_context(
            user_agent=generate_user_agent(),
            ignore_https_errors=True,
            java_script_enabled=False
        )

        try:
            # å¹¶å‘æ§åˆ¶ï¼šçº¿ç¨‹æ•°=åŒæ—¶æ‰“å¼€çš„Tabæ•°ï¼Œ10çº¿ç¨‹=10ä¸ªTabï¼Œå®Œç¾åŒ¹é…ä½ çš„éœ€æ±‚
            semaphore = asyncio.Semaphore(thread_num)
            async def bounded_fetch(url):
                async with semaphore:
                    # âœ… ä¿®æ”¹ç‚¹3ï¼šä¼ å…¥å…¨å±€Contextï¼Œåªåˆ›å»ºTabï¼Œç”¨å®Œå…³Tabï¼Œæ— è¿›ç¨‹æ³„æ¼
                    async with get_playwright_page(global_context) as page:
                        return await fetch_page_async(page, url, progress, args.headers)

            results = await asyncio.gather(*[bounded_fetch(url) for url in urls])

        finally:
            # å…ˆå…³é—­å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå†å…³é—­æµè§ˆå™¨ï¼Œé¡ºåºæ­£ç¡®ï¼Œæ— æ®‹ç•™
            await global_context.close()
            await browser.close()
            progress.close()

    # å¤„ç†è¯·æ±‚ç»“æœï¼ˆç”Ÿæˆscan_infoå¹¶å»é‡ï¼‰
    scan_info_list = []
    # æœªå¤„ç†çš„scan_info_list(ä¸»è¦æ˜¯ç»™excelä¼ å€¼ï¼Œé åŒ—äº†)
    all_next_urls_with_source = []
    all_next_urls = set()
    for html, url, status in results:
        if not html:
            continue

        # ä¿®å¤ç¼–ç 
        html = fix_encoding(html)

        # ç”ŸæˆåŸºç¡€æ‰«æä¿¡æ¯
        parsed = urlparse(url)
        scan_info = {
            "domain": parsed.hostname,
            "url": url,
            "path": parsed.path,
            "port": parsed.port or (443 if parsed.scheme == "https" else 80),
            "status": status,
            "title": get_webpage_title(html),
            "length": len(html),
            "source_code": html,
            "is_valid": 0,
        }
        # åˆtndç»•äº†ä¸€åœˆ
        # scan_info.pop("source_code")
        # all_next_urls_with_source.append(scan_info)
        # scan_info["source_code"] = html

        # å»é‡å¹¶æå–ä¸‹ä¸€å±‚URL
        is_valid, next_urls_without_source = await process_scan_result(scan_info, checker, args)
        if is_valid:
            scan_info["is_valid"] = 1

            next_urls_with_source = {
                "next_urls":next_urls_without_source,
                "sourceURL":url
            }

            all_next_urls_with_source.append(next_urls_with_source)
            all_next_urls.update(next_urls_without_source)

        print(
            f"[bold blue]URL:[/bold blue] {escape(str(scan_info['url']))}\n"  # ç¡®ä¿è½¬ä¸ºå­—ç¬¦ä¸²
            f"\t[bold green]Status:[/bold green] {escape(str(scan_info['status']))}\n"  # çŠ¶æ€ç ï¼ˆæ•´æ•°ï¼‰è½¬å­—ç¬¦ä¸²
            f"\t[bold cyan]Title:[/bold cyan] {escape(str(scan_info['title']))}\n"  # titleå¯èƒ½ä¸ºNoneï¼Œè½¬å­—ç¬¦ä¸²
            f"\t[bold yellow]Content Length:[/bold yellow] {escape(str(scan_info['length']))}\n"  # é•¿åº¦ï¼ˆæ•´æ•°ï¼‰è½¬å­—ç¬¦ä¸²
#            f"\t[bold magenta]Valid Elements:[/bold magenta] {escape(str(scan_info['valid_Element']))}\n"  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
        )
        scan_info_list.append(scan_info)

    return all_next_urls_with_source, scan_info_list, all_next_urls